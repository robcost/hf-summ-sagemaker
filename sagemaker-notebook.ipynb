{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huggingface Sagemaker-sdk - Distributed Training Demo\n",
    "### Distributed Summarization with `transformers` scripts + `Trainer` and `samsum` dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Tutorial](#Tutorial)  \n",
    "2. [Set up a development environment and install sagemaker](#Set-up-a-development-environment-and-install-sagemaker)\n",
    "    1. [Installation](#Installation)  \n",
    "    2. [Development environment](#Development-environment)  \n",
    "    3. [Permissions](#Permissions) \n",
    "4. [Choose ðŸ¤— Transformers `examples/` script](#Choose-%F0%9F%A4%97-Transformers-examples/-script)  \n",
    "1. [Configure distributed training and hyperparameters](#Configure-distributed-training-and-hyperparameters)  \n",
    "2. [Create a `HuggingFace` estimator and start training](#Create-a-HuggingFace-estimator-and-start-training)   \n",
    "3. [Upload the fine-tuned model to huggingface.co](#Upload-the-fine-tuned-model-to-huggingface.co)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial\n",
    "\n",
    "We will use the new [Hugging Face DLCs](https://github.com/aws/deep-learning-containers/tree/master/huggingface) and [Amazon SageMaker extension](https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/sagemaker.huggingface.html#huggingface-estimator) to train a distributed Seq2Seq-transformer model on `summarization` using the `transformers` and `datasets` libraries and upload it afterwards to [huggingface.co](http://huggingface.co) and test it.\n",
    "\n",
    "As [distributed training strategy](https://huggingface.co/transformers/sagemaker.html#distributed-training-data-parallel) we are going to use [SageMaker Data Parallelism](https://aws.amazon.com/blogs/aws/managed-data-parallelism-in-amazon-sagemaker-simplifies-training-on-large-datasets/), which has been built into the [Trainer](https://huggingface.co/transformers/main_classes/trainer.html) API. To use data-parallelism we only have to define the `distribution` parameter in our `HuggingFace` estimator.\n",
    "\n",
    "```python\n",
    "# configuration for running training on smdistributed Data Parallel\n",
    "distribution = {'smdistributed':{'dataparallel':{ 'enabled': True }}}\n",
    "```\n",
    "\n",
    "In this tutorial, we will use an Amazon SageMaker Notebook Instance for running our training job. You can learnÂ [here how to set up a Notebook Instance](https://docs.aws.amazon.com/sagemaker/latest/dg/nbi.html).\n",
    "\n",
    "**What are we going to do:**\n",
    "\n",
    "- Set up a development environment and install sagemaker\n",
    "- Chose ðŸ¤— Transformers `examples/` script\n",
    "- Configure distributed training and hyperparameters\n",
    "- Create a `HuggingFace` estimator and start training\n",
    "- Upload the fine-tuned model to [huggingface.co](http://huggingface.co)\n",
    "- Test inference\n",
    "\n",
    "### Model and Dataset\n",
    "\n",
    "We are going to fine-tune [facebook/bart-base](https://huggingface.co/facebook/bart-base) on the [samsum](https://huggingface.co/datasets/samsum) dataset. *\"BART is sequence-to-sequence model trained with denoising as pretraining objective.\"* [[REF](https://github.com/pytorch/fairseq/blob/master/examples/bart/README.md)]\n",
    "\n",
    "The `samsum` dataset contains about 16k messenger-like conversations with summaries. \n",
    "\n",
    "```python\n",
    "{'id': '13818513',\n",
    " 'summary': 'Amanda baked cookies and will bring Jerry some tomorrow.',\n",
    " 'dialogue': \"Amanda: I baked cookies. Do you want some?\\r\\nJerry: Sure!\\r\\nAmanda: I'll bring you tomorrow :-)\"}\n",
    "```\n",
    "\n",
    "_**NOTE: You can run this demo in Sagemaker Studio, your local machine or Sagemaker Notebook Instances**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up a development environment and install sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "_**Note:**Â The use of Jupyter is optional: We could also launch SageMaker Training jobs from anywhere we have an SDK installed, connectivity to the cloud and appropriate permissions, such as a Laptop, another IDE or a task scheduler like Airflow or AWS Step Functions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"sagemaker>=2.48.0\" \"transformers==4.6.1\" \"datasets[s3]==1.6.2\" --upgrade\n",
    "#!apt install git-lfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.rpm.sh | sudo bash\n",
    "!sudo yum install git-lfs -y\n",
    "!git lfs install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Development environment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**upgrade ipywidgets for `datasets` library and restart kernel, only needed when prerpocessing is done in the notebook**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import IPython\n",
    "!conda install -c conda-forge ipywidgets -y\n",
    "IPython.Application.instance().kernel.do_shutdown(True) # has to restart kernel so changes are used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker.huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_If you are going to use Sagemaker in a local environment. You need access to an IAM Role with the required permissions for Sagemaker. You can find [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) more about it._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IAM role arn used for running training: arn:aws:iam::859830842924:role/service-role/AmazonSageMaker-ExecutionRole-20211013T235227\n",
      "S3 bucket used for storing artifacts: sagemaker-us-east-1-859830842924\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "print(f\"IAM role arn used for running training: {role}\")\n",
    "print(f\"S3 bucket used for storing artifacts: {sess.default_bucket()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose ðŸ¤— Transformers `examples/` script\n",
    "\n",
    "The [ðŸ¤— Transformers repository](https://github.com/huggingface/transformers/tree/master/examples) contains several `examples/`scripts for fine-tuning models on tasks from `language-modeling` to `token-classification`. In our case, we are using the `run_summarization.py` from the `seq2seq/` examples. \n",
    "\n",
    "_**Note**: you can use this tutorial identical to train your model on a different examples script._\n",
    "\n",
    "Since theÂ `HuggingFace`Â Estimator has git support built-in, we can specify aÂ [training script that is stored in a GitHub repository](https://sagemaker.readthedocs.io/en/stable/overview.html#use-scripts-stored-in-a-git-repository)Â asÂ `entry_point`Â andÂ `source_dir`.\n",
    "\n",
    "We are going to use the `transformers 4.4.2` DLC which means we need to configure the `v4.4.2` as the branch to pull the compatible example scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "git_config = {'repo': 'https://github.com/huggingface/transformers.git','branch': 'v4.6.1'} # v4.6.1 is referring to the `transformers_version` you use in the estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure distributed training and hyperparameters\n",
    "\n",
    "Next, we will define our `hyperparameters` and configure our distributed training strategy. As hyperparameter, we can define any [Seq2SeqTrainingArguments](https://huggingface.co/transformers/main_classes/trainer.html#seq2seqtrainingarguments) and the ones defined in [run_summarization.py](https://github.com/huggingface/transformers/tree/master/examples/seq2seq#sequence-to-sequence-training-and-evaluation). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters={'per_device_train_batch_size': 1,\n",
    "                 'per_device_eval_batch_size': 1,\n",
    "                 'model_name_or_path': 'facebook/bart-large-cnn',\n",
    "                 'dataset_name': 'big_patent', #dropping to test S3 based dataset.\n",
    "                 'dataset_config_name': 'g',\n",
    "                 'do_train': True,\n",
    "                 'do_eval': True,\n",
    "                 'do_predict': True,\n",
    "                 'predict_with_generate': True,\n",
    "                 'output_dir': '/opt/ml/model',\n",
    "                 'num_train_epochs': 3,\n",
    "                 'learning_rate': 5e-5,\n",
    "                 'seed': 7,\n",
    "                 'fp16': True,\n",
    "                 'cache_dir': 'opt/ml/input',\n",
    "                 'text_column': 'description',\n",
    "                 'summary_column': 'abstract',\n",
    "                }\n",
    "\n",
    "# configuration for running training on smdistributed Data Parallel\n",
    "distribution = {'smdistributed':{'dataparallel':{ 'enabled': True }}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a `HuggingFace` estimator and start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "      entry_point='run_summarization.py', # script\n",
    "      source_dir='./examples/pytorch/summarization', # relative path to example\n",
    "      git_config=git_config,\n",
    "      instance_type='ml.p3.16xlarge',\n",
    "      instance_count=1,\n",
    "      transformers_version='4.12',\n",
    "      pytorch_version='1.9',\n",
    "      py_version='py38',\n",
    "      role=role,\n",
    "      hyperparameters = hyperparameters,\n",
    "      distribution = distribution,\n",
    "      volume_size=200,\n",
    "      cache_dir='opt/ml/input'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-17 12:28:53 Starting - Starting the training job...\n",
      "2022-01-17 12:29:16 Starting - Launching requested ML instancesProfilerReport-1642422525: InProgress\n",
      ".........\n",
      "2022-01-17 12:30:43 Starting - Preparing the instances for training.........\n",
      "2022-01-17 12:32:17 Downloading - Downloading input data...\n",
      "2022-01-17 12:32:37 Training - Downloading the training image..............................\n",
      "2022-01-17 12:37:48 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-01-17 12:37:48,981 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-01-17 12:37:49,057 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-01-17 12:37:49,063 sagemaker_pytorch_container.training INFO     Invoking SMDataParallel\u001b[0m\n",
      "\u001b[34m2022-01-17 12:37:49,063 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-01-17 12:37:49,799 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: datasets>=1.1.3 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (1.15.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sentencepiece!=0.1.92 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (0.1.96)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 3)) (3.19.0)\u001b[0m\n",
      "\u001b[34mCollecting rouge-score\u001b[0m\n",
      "\u001b[34mDownloading rouge_score-0.0.4-py2.py3-none-any.whl (22 kB)\u001b[0m\n",
      "\u001b[34mCollecting nltk\u001b[0m\n",
      "\u001b[34mDownloading nltk-3.6.7-py3-none-any.whl (1.5 MB)\u001b[0m\n",
      "\u001b[34mCollecting py7zr\u001b[0m\n",
      "\u001b[34mDownloading py7zr-0.17.2-py3-none-any.whl (68 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.3 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 7)) (1.9.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (21.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.8/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (2.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.8/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (0.70.12.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.8/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (2.26.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.8/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (2021.10.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (1.2.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.8/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (3.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (1.19.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (5.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill in /opt/conda/lib/python3.8/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (0.3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.8/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (4.62.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.8/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (0.1.0)\u001b[0m\n",
      "\u001b[34mCollecting absl-py\u001b[0m\n",
      "\u001b[34mDownloading absl_py-1.0.0-py3-none-any.whl (126 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.8/site-packages (from rouge-score->-r requirements.txt (line 4)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (from nltk->-r requirements.txt (line 5)) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.8/site-packages (from nltk->-r requirements.txt (line 5)) (8.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.8/site-packages (from nltk->-r requirements.txt (line 5)) (2021.11.2)\u001b[0m\n",
      "\u001b[34mCollecting texttable\u001b[0m\n",
      "\u001b[34mDownloading texttable-1.6.4-py2.py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mCollecting brotli>=1.0.9\u001b[0m\n",
      "\u001b[34mDownloading Brotli-1.0.9-cp38-cp38-manylinux1_x86_64.whl (357 kB)\u001b[0m\n",
      "\u001b[34mCollecting pyzstd>=0.14.4\u001b[0m\n",
      "\u001b[34mDownloading pyzstd-0.15.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\u001b[0m\n",
      "\u001b[34mCollecting pybcj>=0.5.0\u001b[0m\n",
      "\u001b[34mDownloading pybcj-0.5.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (48 kB)\u001b[0m\n",
      "\u001b[34mCollecting pycryptodomex>=3.6.6\u001b[0m\n",
      "\u001b[34mDownloading pycryptodomex-3.12.0-cp35-abi3-manylinux2010_x86_64.whl (2.0 MB)\u001b[0m\n",
      "\u001b[34mCollecting multivolumefile>=0.2.3\u001b[0m\n",
      "\u001b[34mDownloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\u001b[0m\n",
      "\u001b[34mCollecting pyppmd>=0.17.0\u001b[0m\n",
      "\u001b[34mDownloading pyppmd-0.17.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (127 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch>=1.3->-r requirements.txt (line 7)) (3.10.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets>=1.1.3->-r requirements.txt (line 1)) (3.3.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets>=1.1.3->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->datasets>=1.1.3->-r requirements.txt (line 1)) (2.4.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets>=1.1.3->-r requirements.txt (line 1)) (1.26.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets>=1.1.3->-r requirements.txt (line 1)) (2.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets>=1.1.3->-r requirements.txt (line 1)) (2021.10.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets>=1.1.3->-r requirements.txt (line 1)) (2.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets>=1.1.3->-r requirements.txt (line 1)) (1.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets>=1.1.3->-r requirements.txt (line 1)) (5.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets>=1.1.3->-r requirements.txt (line 1)) (4.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets>=1.1.3->-r requirements.txt (line 1)) (1.7.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets>=1.1.3->-r requirements.txt (line 1)) (21.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets>=1.1.3->-r requirements.txt (line 1)) (1.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets>=1.1.3->-r requirements.txt (line 1)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets>=1.1.3->-r requirements.txt (line 1)) (2021.3)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: texttable, pyzstd, pyppmd, pycryptodomex, pybcj, nltk, multivolumefile, brotli, absl-py, rouge-score, py7zr\u001b[0m\n",
      "\u001b[34mSuccessfully installed absl-py-1.0.0 brotli-1.0.9 multivolumefile-0.2.3 nltk-3.6.7 py7zr-0.17.2 pybcj-0.5.0 pycryptodomex-3.12.0 pyppmd-0.17.3 pyzstd-0.15.1 rouge-score-0.0.4 texttable-1.6.4\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2022-01-17 12:37:55,461 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[34m2022-01-17 12:37:55,461 sagemaker-training-toolkit INFO     Creating SSH daemon.\u001b[0m\n",
      "\u001b[34m2022-01-17 12:37:55,464 sagemaker-training-toolkit INFO     Waiting for MPI workers to establish their SSH connections\u001b[0m\n",
      "\u001b[34m2022-01-17 12:37:55,464 sagemaker-training-toolkit INFO     Network interface name: eth0\u001b[0m\n",
      "\u001b[34m2022-01-17 12:37:55,464 sagemaker-training-toolkit INFO     Host: ['algo-1']\u001b[0m\n",
      "\u001b[34m2022-01-17 12:37:55,465 sagemaker-training-toolkit INFO     instance type: ml.p3.16xlarge\u001b[0m\n",
      "\u001b[34m2022-01-17 12:37:55,540 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_distributed_dataparallel_custom_mpi_options\": \"\",\n",
      "        \"sagemaker_distributed_dataparallel_enabled\": true,\n",
      "        \"sagemaker_instance_type\": \"ml.p3.16xlarge\"\n",
      "    },\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"per_device_eval_batch_size\": 1,\n",
      "        \"seed\": 7,\n",
      "        \"do_predict\": true,\n",
      "        \"text_column\": \"description\",\n",
      "        \"num_train_epochs\": 3,\n",
      "        \"do_eval\": true,\n",
      "        \"cache_dir\": \"opt/ml/input\",\n",
      "        \"dataset_config_name\": \"g\",\n",
      "        \"output_dir\": \"/opt/ml/model\",\n",
      "        \"fp16\": true,\n",
      "        \"predict_with_generate\": true,\n",
      "        \"do_train\": true,\n",
      "        \"dataset_name\": \"big_patent\",\n",
      "        \"per_device_train_batch_size\": 1,\n",
      "        \"learning_rate\": 5e-05,\n",
      "        \"model_name_or_path\": \"facebook/bart-large-cnn\",\n",
      "        \"summary_column\": \"abstract\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"huggingface-pytorch-training-2022-01-17-12-28-45-434\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-859830842924/huggingface-pytorch-training-2022-01-17-12-28-45-434/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run_summarization\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 64,\n",
      "    \"num_gpus\": 8,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run_summarization.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"cache_dir\":\"opt/ml/input\",\"dataset_config_name\":\"g\",\"dataset_name\":\"big_patent\",\"do_eval\":true,\"do_predict\":true,\"do_train\":true,\"fp16\":true,\"learning_rate\":5e-05,\"model_name_or_path\":\"facebook/bart-large-cnn\",\"num_train_epochs\":3,\"output_dir\":\"/opt/ml/model\",\"per_device_eval_batch_size\":1,\"per_device_train_batch_size\":1,\"predict_with_generate\":true,\"seed\":7,\"summary_column\":\"abstract\",\"text_column\":\"description\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=run_summarization.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_distributed_dataparallel_custom_mpi_options\":\"\",\"sagemaker_distributed_dataparallel_enabled\":true,\"sagemaker_instance_type\":\"ml.p3.16xlarge\"}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=run_summarization\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=64\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-859830842924/huggingface-pytorch-training-2022-01-17-12-28-45-434/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_distributed_dataparallel_custom_mpi_options\":\"\",\"sagemaker_distributed_dataparallel_enabled\":true,\"sagemaker_instance_type\":\"ml.p3.16xlarge\"},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"cache_dir\":\"opt/ml/input\",\"dataset_config_name\":\"g\",\"dataset_name\":\"big_patent\",\"do_eval\":true,\"do_predict\":true,\"do_train\":true,\"fp16\":true,\"learning_rate\":5e-05,\"model_name_or_path\":\"facebook/bart-large-cnn\",\"num_train_epochs\":3,\"output_dir\":\"/opt/ml/model\",\"per_device_eval_batch_size\":1,\"per_device_train_batch_size\":1,\"predict_with_generate\":true,\"seed\":7,\"summary_column\":\"abstract\",\"text_column\":\"description\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"huggingface-pytorch-training-2022-01-17-12-28-45-434\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-859830842924/huggingface-pytorch-training-2022-01-17-12-28-45-434/source/sourcedir.tar.gz\",\"module_name\":\"run_summarization\",\"network_interface_name\":\"eth0\",\"num_cpus\":64,\"num_gpus\":8,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run_summarization.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--cache_dir\",\"opt/ml/input\",\"--dataset_config_name\",\"g\",\"--dataset_name\",\"big_patent\",\"--do_eval\",\"True\",\"--do_predict\",\"True\",\"--do_train\",\"True\",\"--fp16\",\"True\",\"--learning_rate\",\"5e-05\",\"--model_name_or_path\",\"facebook/bart-large-cnn\",\"--num_train_epochs\",\"3\",\"--output_dir\",\"/opt/ml/model\",\"--per_device_eval_batch_size\",\"1\",\"--per_device_train_batch_size\",\"1\",\"--predict_with_generate\",\"True\",\"--seed\",\"7\",\"--summary_column\",\"abstract\",\"--text_column\",\"description\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_EVAL_BATCH_SIZE=1\u001b[0m\n",
      "\u001b[34mSM_HP_SEED=7\u001b[0m\n",
      "\u001b[34mSM_HP_DO_PREDICT=true\u001b[0m\n",
      "\u001b[34mSM_HP_TEXT_COLUMN=description\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_TRAIN_EPOCHS=3\u001b[0m\n",
      "\u001b[34mSM_HP_DO_EVAL=true\u001b[0m\n",
      "\u001b[34mSM_HP_CACHE_DIR=opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_HP_DATASET_CONFIG_NAME=g\u001b[0m\n",
      "\u001b[34mSM_HP_OUTPUT_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_HP_FP16=true\u001b[0m\n",
      "\u001b[34mSM_HP_PREDICT_WITH_GENERATE=true\u001b[0m\n",
      "\u001b[34mSM_HP_DO_TRAIN=true\u001b[0m\n",
      "\u001b[34mSM_HP_DATASET_NAME=big_patent\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=1\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=5e-05\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME_OR_PATH=facebook/bart-large-cnn\u001b[0m\n",
      "\u001b[34mSM_HP_SUMMARY_COLUMN=abstract\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34mmpirun --host algo-1 -np 8 --allow-run-as-root --tag-output --oversubscribe -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -mca plm_rsh_num_concurrent 1 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x SMDATAPARALLEL_USE_SINGLENODE=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1 -x LD_PRELOAD=/opt/conda/lib/python3.8/site-packages/gethostname.cpython-38-x86_64-linux-gnu.so smddprun /opt/conda/bin/python3.8 -m mpi4py run_summarization.py --cache_dir opt/ml/input --dataset_config_name g --dataset_name big_patent --do_eval True --do_predict True --do_train True --fp16 True --learning_rate 5e-05 --model_name_or_path facebook/bart-large-cnn --num_train_epochs 3 --output_dir /opt/ml/model --per_device_eval_batch_size 1 --per_device_train_batch_size 1 --predict_with_generate True --seed 7 --summary_column abstract --text_column description\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:941:941 [0] NCCL INFO Bootstrap : Using [0]eth0:10.2.199.101<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:941:941 [0] find_ofi_provider:543 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:941:941 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:941:941 [0] NCCL INFO NET/Socket : Using [0]eth0:10.2.199.101<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:941:941 [0] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:NCCL version 2.7.8+cuda11.1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:47:47 [2] NCCL INFO Bootstrap : Using [0]eth0:10.2.199.101<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:51:51 [4] NCCL INFO Bootstrap : Using [0]eth0:10.2.199.101<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:49:49 [3] NCCL INFO Bootstrap : Using [0]eth0:10.2.199.101<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:54:54 [6] NCCL INFO Bootstrap : Using [0]eth0:10.2.199.101<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:55:55 [7] NCCL INFO Bootstrap : Using [0]eth0:10.2.199.101<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:46:46 [1] NCCL INFO Bootstrap : Using [0]eth0:10.2.199.101<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:53:53 [5] NCCL INFO Bootstrap : Using [0]eth0:10.2.199.101<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:51:51 [4] find_ofi_provider:543 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:49:49 [3] find_ofi_provider:543 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:54:54 [6] find_ofi_provider:543 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:55:55 [7] find_ofi_provider:543 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:47:47 [2] find_ofi_provider:543 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:54:54 [6] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:51:51 [4] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:49:49 [3] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:55:55 [7] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:47:47 [2] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:55:55 [7] NCCL INFO NET/Socket : Using [0]eth0:10.2.199.101<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:55:55 [7] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:49:49 [3] NCCL INFO NET/Socket : Using [0]eth0:10.2.199.101<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:49:49 [3] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:54:54 [6] NCCL INFO NET/Socket : Using [0]eth0:10.2.199.101<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:54:54 [6] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:51:51 [4] NCCL INFO NET/Socket : Using [0]eth0:10.2.199.101<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:51:51 [4] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:47:47 [2] NCCL INFO NET/Socket : Using [0]eth0:10.2.199.101<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:47:47 [2] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:53:53 [5] find_ofi_provider:543 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:46:46 [1] find_ofi_provider:543 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:53:53 [5] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:46:46 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:53:53 [5] NCCL INFO NET/Socket : Using [0]eth0:10.2.199.101<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:46:46 [1] NCCL INFO NET/Socket : Using [0]eth0:10.2.199.101<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:46:46 [1] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:53:53 [5] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:46:46 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:46:46 [1] NCCL INFO Trees [0] 5/-1/-1->1->2|2->1->5/-1/-1 [1] 5/-1/-1->1->2|2->1->5/-1/-1 [2] 2/-1/-1->1->5|5->1->2/-1/-1 [3] 2/-1/-1->1->5|5->1->2/-1/-1 [4] 3/-1/-1->1->0|0->1->3/-1/-1 [5] -1/-1/-1->1->3|3->1->-1/-1/-1 [6] 5/-1/-1->1->2|2->1->5/-1/-1 [7] 5/-1/-1->1->2|2->1->5/-1/-1 [8] 2/-1/-1->1->5|5->1->2/-1/-1 [9] 2/-1/-1->1->5|5->1->2/-1/-1 [10] 3/-1/-1->1->0|0->1->3/-1/-1 [11] -1/-1/-1->1->3|3->1->-1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:51:51 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:47:47 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:47:47 [2] NCCL INFO Trees [0] 1/-1/-1->2->3|3->2->1/-1/-1 [1] 1/-1/-1->2->3|3->2->1/-1/-1 [2] 3/-1/-1->2->1|1->2->3/-1/-1 [3] 3/-1/-1->2->1|1->2->3/-1/-1 [4] -1/-1/-1->2->6|6->2->-1/-1/-1 [5] 6/-1/-1->2->0|0->2->6/-1/-1 [6] 1/-1/-1->2->3|3->2->1/-1/-1 [7] 1/-1/-1->2->3|3->2->1/-1/-1 [8] 3/-1/-1->2->1|1->2->3/-1/-1 [9] 3/-1/-1->2->1|1->2->3/-1/-1 [10] -1/-1/-1->2->6|6->2->-1/-1/-1 [11] 6/-1/-1->2->0|0->2->6/-1/-1\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:53:53 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:49:49 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:49:49 [3] NCCL INFO Trees [0] 2/-1/-1->3->0|0->3->2/-1/-1 [1] 2/-1/-1->3->0|0->3->2/-1/-1 [2] -1/-1/-1->3->2|2->3->-1/-1/-1 [3] -1/-1/-1->3->2|2->3->-1/-1/-1 [4] 7/-1/-1->3->1|1->3->7/-1/-1 [5] 1/-1/-1->3->7|7->3->1/-1/-1 [6] 2/-1/-1->3->0|0->3->2/-1/-1 [7] 2/-1/-1->3->0|0->3->2/-1/-1 [8] -1/-1/-1->3->2|2->3->-1/-1/-1 [9] -1/-1/-1->3->2|2->3->-1/-1/-1 [10] 7/-1/-1->3->1|1->3->7/-1/-1 [11] 1/-1/-1->3->7|7->3->1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:51:51 [4] NCCL INFO Trees [0] -1/-1/-1->4->7|7->4->-1/-1/-1 [1] -1/-1/-1->4->7|7->4->-1/-1/-1 [2] 7/-1/-1->4->0|0->4->7/-1/-1 [3] 7/-1/-1->4->0|0->4->7/-1/-1 [4] 6/-1/-1->4->5|5->4->6/-1/-1 [5] 5/-1/-1->4->6|6->4->5/-1/-1 [6] -1/-1/-1->4->7|7->4->-1/-1/-1 [7] -1/-1/-1->4->7|7->4->-1/-1/-1 [8] 7/-1/-1->4->0|0->4->7/-1/-1 [9] 7/-1/-1->4->0|0->4->7/-1/-1 [10] 6/-1/-1->4->5|5->4->6/-1/-1 [11] 5/-1/-1->4->6|6->4->5/-1/-1\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:53:53 [5] NCCL INFO Trees [0] 6/-1/-1->5->1|1->5->6/-1/-1 [1] 6/-1/-1->5->1|1->5->6/-1/-1 [2] 1/-1/-1->5->6|6->5->1/-1/-1 [3] 1/-1/-1->5->6|6->5->1/-1/-1 [4] 4/-1/-1->5->7|7->5->4/-1/-1 [5] 7/-1/-1->5->4|4->5->7/-1/-1 [6] 6/-1/-1->5->1|1->5->6/-1/-1 [7] 6/-1/-1->5->1|1->5->6/-1/-1 [8] 1/-1/-1->5->6|6->5->1/-1/-1 [9] 1/-1/-1->5->6|6->5->1/-1/-1 [10] 4/-1/-1->5->7|7->5->4/-1/-1 [11] 7/-1/-1->5->4|4->5->7/-1/-1\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:54:54 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:55:55 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:54:54 [6] NCCL INFO Trees [0] 7/-1/-1->6->5|5->6->7/-1/-1 [1] 7/-1/-1->6->5|5->6->7/-1/-1 [2] 5/-1/-1->6->7|7->6->5/-1/-1 [3] 5/-1/-1->6->7|7->6->5/-1/-1 [4] 2/-1/-1->6->4|4->6->2/-1/-1 [5] 4/-1/-1->6->2|2->6->4/-1/-1 [6] 7/-1/-1->6->5|5->6->7/-1/-1 [7] 7/-1/-1->6->5|5->6->7/-1/-1 [8] 5/-1/-1->6->7|7->6->5/-1/-1 [9] 5/-1/-1->6->7|7->6->5/-1/-1 [10] 2/-1/-1->6->4|4->6->2/-1/-1 [11] 4/-1/-1->6->2|2->6->4/-1/-1\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:55:55 [7] NCCL INFO Trees [0] 4/-1/-1->7->6|6->7->4/-1/-1 [1] 4/-1/-1->7->6|6->7->4/-1/-1 [2] 6/-1/-1->7->4|4->7->6/-1/-1 [3] 6/-1/-1->7->4|4->7->6/-1/-1 [4] 5/-1/-1->7->3|3->7->5/-1/-1 [5] 3/-1/-1->7->5|5->7->3/-1/-1 [6] 4/-1/-1->7->6|6->7->4/-1/-1 [7] 4/-1/-1->7->6|6->7->4/-1/-1 [8] 6/-1/-1->7->4|4->7->6/-1/-1 [9] 6/-1/-1->7->4|4->7->6/-1/-1 [10] 5/-1/-1->7->3|3->7->5/-1/-1 [11] 3/-1/-1->7->5|5->7->3/-1/-1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:941:941 [0] NCCL INFO Channel 00/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:941:941 [0] NCCL INFO Channel 01/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:941:941 [0] NCCL INFO Channel 02/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:941:941 [0] NCCL INFO Channel 03/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:941:941 [0] NCCL INFO Channel 04/12 :    0   1   3   7   5   4   6   2\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:941:941 [0] NCCL INFO Channel 05/12 :    0   2   6   4   5   7   3   1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:941:941 [0] NCCL INFO Channel 06/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:941:941 [0] NCCL INFO Channel 07/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:941:941 [0] NCCL INFO Channel 08/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:941:941 [0] NCCL INFO Channel 09/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:941:941 [0] NCCL INFO Channel 10/12 :    0   1   3   7   5   4   6   2\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:941:941 [0] NCCL INFO Channel 11/12 :    0   2   6   4   5   7   3   1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:941:941 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:941:941 [0] NCCL INFO Trees [0] 3/-1/-1->0->-1|-1->0->3/-1/-1 [1] 3/-1/-1->0->-1|-1->0->3/-1/-1 [2] 4/-1/-1->0->-1|-1->0->4/-1/-1 [3] 4/-1/-1->0->-1|-1->0->4/-1/-1 [4] 1/-1/-1->0->-1|-1->0->1/-1/-1 [5] 2/-1/-1->0->-1|-1->0->2/-1/-1 [6] 3/-1/-1->0->-1|-1->0->3/-1/-1 [7] 3/-1/-1->0->-1|-1->0->3/-1/-1 [8] 4/-1/-1->0->-1|-1->0->4/-1/-1 [9] 4/-1/-1->0->-1|-1->0->4/-1/-1 [10] 1/-1/-1->0->-1|-1->0->1/-1/-1 [11] 2/-1/-1->0->-1|-1->0->2/-1/-1\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:46:46 [1] NCCL INFO Channel 00 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:49:49 [3] NCCL INFO Channel 00 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:47:47 [2] NCCL INFO Channel 00 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:51:51 [4] NCCL INFO Channel 00 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:53:53 [5] NCCL INFO Channel 00 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:54:54 [6] NCCL INFO Channel 00 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:55:55 [7] NCCL INFO Channel 00 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:941:941 [0] NCCL INFO Channel 00 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:51:51 [4] NCCL INFO Channel 00 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:46:46 [1] NCCL INFO Channel 00 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:49:49 [3] NCCL INFO Channel 00 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:47:47 [2] NCCL INFO Channel 00 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:53:53 [5] NCCL INFO Channel 00 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:54:54 [6] NCCL INFO Channel 00 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:55:55 [7] NCCL INFO Channel 00 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:51:51 [4] NCCL INFO Channel 01 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:941:941 [0] NCCL INFO Channel 01 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:46:46 [1] NCCL INFO Channel 01 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:49:49 [3] NCCL INFO Channel 01 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:47:47 [2] NCCL INFO Channel 01 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:53:53 [5] NCCL INFO Channel 01 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:54:54 [6] NCCL INFO Channel 01 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:55:55 [7] NCCL INFO Channel 01 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:51:51 [4] NCCL INFO Channel 01 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:49:49 [3] NCCL INFO Channel 01 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:46:46 [1] NCCL INFO Channel 01 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:47:47 [2] NCCL INFO Channel 01 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:53:53 [5] NCCL INFO Channel 01 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:54:54 [6] NCCL INFO Channel 01 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:55:55 [7] NCCL INFO Channel 01 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:51:51 [4] NCCL INFO Channel 02 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:941:941 [0] NCCL INFO Channel 02 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:46:46 [1] NCCL INFO Channel 02 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:49:49 [3] NCCL INFO Channel 02 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:47:47 [2] NCCL INFO Channel 02 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:53:53 [5] NCCL INFO Channel 02 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:54:54 [6] NCCL INFO Channel 02 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:55:55 [7] NCCL INFO Channel 02 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:49:49 [3] NCCL INFO Channel 02 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:51:51 [4] NCCL INFO Channel 02 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:46:46 [1] NCCL INFO Channel 02 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:47:47 [2] NCCL INFO Channel 02 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:53:53 [5] NCCL INFO Channel 02 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:54:54 [6] NCCL INFO Channel 02 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:55:55 [7] NCCL INFO Channel 02 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:49:49 [3] NCCL INFO Channel 03 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:941:941 [0] NCCL INFO Channel 03 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:51:51 [4] NCCL INFO Channel 03 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:46:46 [1] NCCL INFO Channel 03 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:47:47 [2] NCCL INFO Channel 03 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:53:53 [5] NCCL INFO Channel 03 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:54:54 [6] NCCL INFO Channel 03 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:55:55 [7] NCCL INFO Channel 03 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:49:49 [3] NCCL INFO Channel 03 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:51:51 [4] NCCL INFO Channel 03 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:46:46 [1] NCCL INFO Channel 03 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:47:47 [2] NCCL INFO Channel 03 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:53:53 [5] NCCL INFO Channel 03 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:54:54 [6] NCCL INFO Channel 03 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:55:55 [7] NCCL INFO Channel 03 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:49:49 [3] NCCL INFO Channel 04 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:941:941 [0] NCCL INFO Channel 04 : 0[170] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:51:51 [4] NCCL INFO Channel 04 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:47:47 [2] NCCL INFO Channel 04 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:46:46 [1] NCCL INFO Channel 04 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:54:54 [6] NCCL INFO Channel 04 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:53:53 [5] NCCL INFO Channel 04 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:55:55 [7] NCCL INFO Channel 04 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:47:47 [2] NCCL INFO Channel 04 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:49:49 [3] NCCL INFO Channel 04 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:51:51 [4] NCCL INFO Channel 04 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:46:46 [1] NCCL INFO Channel 04 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:53:53 [5] NCCL INFO Channel 04 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:54:54 [6] NCCL INFO Channel 04 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:55:55 [7] NCCL INFO Channel 04 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:47:47 [2] NCCL INFO Channel 05 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:941:941 [0] NCCL INFO Channel 05 : 0[170] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:51:51 [4] NCCL INFO Channel 05 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:49:49 [3] NCCL INFO Channel 05 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:46:46 [1] NCCL INFO Channel 05 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:54:54 [6] NCCL INFO Channel 05 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:53:53 [5] NCCL INFO Channel 05 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:55:55 [7] NCCL INFO Channel 05 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:46:46 [1] NCCL INFO Channel 05 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:47:47 [2] NCCL INFO Channel 05 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:51:51 [4] NCCL INFO Channel 05 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:49:49 [3] NCCL INFO Channel 05 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:54:54 [6] NCCL INFO Channel 05 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:53:53 [5] NCCL INFO Channel 05 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:55:55 [7] NCCL INFO Channel 05 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:46:46 [1] NCCL INFO Channel 06 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:941:941 [0] NCCL INFO Channel 06 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:47:47 [2] NCCL INFO Channel 06 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:51:51 [4] NCCL INFO Channel 06 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:49:49 [3] NCCL INFO Channel 06 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:54:54 [6] NCCL INFO Channel 06 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:53:53 [5] NCCL INFO Channel 06 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:55:55 [7] NCCL INFO Channel 06 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:51:51 [4] NCCL INFO Channel 06 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:46:46 [1] NCCL INFO Channel 06 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:47:47 [2] NCCL INFO Channel 06 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:49:49 [3] NCCL INFO Channel 06 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:54:54 [6] NCCL INFO Channel 06 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:53:53 [5] NCCL INFO Channel 06 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:55:55 [7] NCCL INFO Channel 06 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:51:51 [4] NCCL INFO Channel 07 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:941:941 [0] NCCL INFO Channel 07 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:49:49 [3] NCCL INFO Channel 07 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:47:47 [2] NCCL INFO Channel 07 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:46:46 [1] NCCL INFO Channel 07 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:54:54 [6] NCCL INFO Channel 07 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:53:53 [5] NCCL INFO Channel 07 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:55:55 [7] NCCL INFO Channel 07 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:51:51 [4] NCCL INFO Channel 07 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:49:49 [3] NCCL INFO Channel 07 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:47:47 [2] NCCL INFO Channel 07 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:46:46 [1] NCCL INFO Channel 07 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:54:54 [6] NCCL INFO Channel 07 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:55:55 [7] NCCL INFO Channel 07 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:53:53 [5] NCCL INFO Channel 07 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:51:51 [4] NCCL INFO Channel 08 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:941:941 [0] NCCL INFO Channel 08 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:49:49 [3] NCCL INFO Channel 08 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:47:47 [2] NCCL INFO Channel 08 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:46:46 [1] NCCL INFO Channel 08 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:54:54 [6] NCCL INFO Channel 08 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:55:55 [7] NCCL INFO Channel 08 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:53:53 [5] NCCL INFO Channel 08 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:49:49 [3] NCCL INFO Channel 08 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:51:51 [4] NCCL INFO Channel 08 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:47:47 [2] NCCL INFO Channel 08 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:46:46 [1] NCCL INFO Channel 08 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:54:54 [6] NCCL INFO Channel 08 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:55:55 [7] NCCL INFO Channel 08 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:53:53 [5] NCCL INFO Channel 08 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:49:49 [3] NCCL INFO Channel 09 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:941:941 [0] NCCL INFO Channel 09 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:47:47 [2] NCCL INFO Channel 09 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:51:51 [4] NCCL INFO Channel 09 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:46:46 [1] NCCL INFO Channel 09 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:54:54 [6] NCCL INFO Channel 09 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:55:55 [7] NCCL INFO Channel 09 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:53:53 [5] NCCL INFO Channel 09 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:49:49 [3] NCCL INFO Channel 09 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:47:47 [2] NCCL INFO Channel 09 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:51:51 [4] NCCL INFO Channel 09 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:46:46 [1] NCCL INFO Channel 09 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:54:54 [6] NCCL INFO Channel 09 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:55:55 [7] NCCL INFO Channel 09 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:53:53 [5] NCCL INFO Channel 09 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:49:49 [3] NCCL INFO Channel 10 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:941:941 [0] NCCL INFO Channel 10 : 0[170] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:47:47 [2] NCCL INFO Channel 10 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:51:51 [4] NCCL INFO Channel 10 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:46:46 [1] NCCL INFO Channel 10 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:54:54 [6] NCCL INFO Channel 10 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:55:55 [7] NCCL INFO Channel 10 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:53:53 [5] NCCL INFO Channel 10 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:47:47 [2] NCCL INFO Channel 10 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:49:49 [3] NCCL INFO Channel 10 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:51:51 [4] NCCL INFO Channel 10 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:46:46 [1] NCCL INFO Channel 10 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:54:54 [6] NCCL INFO Channel 10 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:55:55 [7] NCCL INFO Channel 10 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:53:53 [5] NCCL INFO Channel 10 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:47:47 [2] NCCL INFO Channel 11 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:941:941 [0] NCCL INFO Channel 11 : 0[170] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:51:51 [4] NCCL INFO Channel 11 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:49:49 [3] NCCL INFO Channel 11 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:46:46 [1] NCCL INFO Channel 11 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:54:54 [6] NCCL INFO Channel 11 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:55:55 [7] NCCL INFO Channel 11 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:53:53 [5] NCCL INFO Channel 11 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:46:46 [1] NCCL INFO Channel 11 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:47:47 [2] NCCL INFO Channel 11 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:49:49 [3] NCCL INFO Channel 11 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:51:51 [4] NCCL INFO Channel 11 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:54:54 [6] NCCL INFO Channel 11 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:55:55 [7] NCCL INFO Channel 11 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:46:46 [1] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:53:53 [5] NCCL INFO Channel 11 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:46:46 [1] NCCL INFO comm 0x560296aafd80 rank 1 nranks 8 cudaDev 1 busId 180 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:941:941 [0] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:941:941 [0] NCCL INFO comm 0x5575452d0890 rank 0 nranks 8 cudaDev 0 busId 170 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:47:47 [2] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:47:47 [2] NCCL INFO comm 0x5627c0bebad0 rank 2 nranks 8 cudaDev 2 busId 190 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:49:49 [3] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:51:51 [4] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:49:49 [3] NCCL INFO comm 0x559df88ae220 rank 3 nranks 8 cudaDev 3 busId 1a0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:54:54 [6] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:51:51 [4] NCCL INFO comm 0x5561cb808260 rank 4 nranks 8 cudaDev 4 busId 1b0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:54:54 [6] NCCL INFO comm 0x557e7264a680 rank 6 nranks 8 cudaDev 6 busId 1d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:55:55 [7] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:53:53 [5] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:55:55 [7] NCCL INFO comm 0x55a085ad9580 rank 7 nranks 8 cudaDev 7 busId 1e0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:53:53 [5] NCCL INFO comm 0x563f6ca29260 rank 5 nranks 8 cudaDev 5 busId 1c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:941:941 [0] NCCL INFO Channel 00/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:941:941 [0] NCCL INFO Channel 01/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:941:941 [0] NCCL INFO Channel 02/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:941:941 [0] NCCL INFO Channel 03/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:941:941 [0] NCCL INFO Channel 04/12 :    0   1   3   7   5   4   6   2\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:941:941 [0] NCCL INFO Channel 05/12 :    0   2   6   4   5   7   3   1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:941:941 [0] NCCL INFO Channel 06/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:46:46 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:46:46 [1] NCCL INFO Trees [0] 5/-1/-1->1->2|2->1->5/-1/-1 [1] 5/-1/-1->1->2|2->1->5/-1/-1 [2] 2/-1/-1->1->5|5->1->2/-1/-1 [3] 2/-1/-1->1->5|5->1->2/-1/-1 [4] 3/-1/-1->1->0|0->1->3/-1/-1 [5] -1/-1/-1->1->3|3->1->-1/-1/-1 [6] 5/-1/-1->1->2|2->1->5/-1/-1 [7] 5/-1/-1->1->2|2->1->5/-1/-1 [8] 2/-1/-1->1->5|5->1->2/-1/-1 [9] 2/-1/-1->1->5|5->1->2/-1/-1 [10] 3/-1/-1->1->0|0->1->3/-1/-1 [11] -1/-1/-1->1->3|3->1->-1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:55:55 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:55:55 [7] NCCL INFO Trees [0] 4/-1/-1->7->6|6->7->4/-1/-1 [1] 4/-1/-1->7->6|6->7->4/-1/-1 [2] 6/-1/-1->7->4|4->7->6/-1/-1 [3] 6/-1/-1->7->4|4->7->6/-1/-1 [4] 5/-1/-1->7->3|3->7->5/-1/-1 [5] 3/-1/-1->7->5|5->7->3/-1/-1 [6] 4/-1/-1->7->6|6->7->4/-1/-1 [7] 4/-1/-1->7->6|6->7->4/-1/-1 [8] 6/-1/-1->7->4|4->7->6/-1/-1 [9] 6/-1/-1->7->4|4->7->6/-1/-1 [10] 5/-1/-1->7->3|3->7->5/-1/-1 [11] 3/-1/-1->7->5|5->7->3/-1/-1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:941:941 [0] NCCL INFO Channel 07/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:941:941 [0] NCCL INFO Channel 08/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:47:47 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:47:47 [2] NCCL INFO Trees [0] 1/-1/-1->2->3|3->2->1/-1/-1 [1] 1/-1/-1->2->3|3->2->1/-1/-1 [2] 3/-1/-1->2->1|1->2->3/-1/-1 [3] 3/-1/-1->2->1|1->2->3/-1/-1 [4] -1/-1/-1->2->6|6->2->-1/-1/-1 [5] 6/-1/-1->2->0|0->2->6/-1/-1 [6] 1/-1/-1->2->3|3->2->1/-1/-1 [7] 1/-1/-1->2->3|3->2->1/-1/-1 [8] 3/-1/-1->2->1|1->2->3/-1/-1 [9] 3/-1/-1->2->1|1->2->3/-1/-1 [10] -1/-1/-1->2->6|6->2->-1/-1/-1 [11] 6/-1/-1->2->0|0->2->6/-1/-1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:941:941 [0] NCCL INFO Channel 09/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:941:941 [0] NCCL INFO Channel 10/12 :    0   1   3   7   5   4   6   2\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:941:941 [0] NCCL INFO Channel 11/12 :    0   2   6   4   5   7   3   1\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:49:49 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:51:51 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:49:49 [3] NCCL INFO Trees [0] 2/-1/-1->3->0|0->3->2/-1/-1 [1] 2/-1/-1->3->0|0->3->2/-1/-1 [2] -1/-1/-1->3->2|2->3->-1/-1/-1 [3] -1/-1/-1->3->2|2->3->-1/-1/-1 [4] 7/-1/-1->3->1|1->3->7/-1/-1 [5] 1/-1/-1->3->7|7->3->1/-1/-1 [6] 2/-1/-1->3->0|0->3->2/-1/-1 [7] 2/-1/-1->3->0|0->3->2/-1/-1 [8] -1/-1/-1->3->2|2->3->-1/-1/-1 [9] -1/-1/-1->3->2|2->3->-1/-1/-1 [10] 7/-1/-1->3->1|1->3->7/-1/-1 [11] 1/-1/-1->3->7|7->3->1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:51:51 [4] NCCL INFO Trees [0] -1/-1/-1->4->7|7->4->-1/-1/-1 [1] -1/-1/-1->4->7|7->4->-1/-1/-1 [2] 7/-1/-1->4->0|0->4->7/-1/-1 [3] 7/-1/-1->4->0|0->4->7/-1/-1 [4] 6/-1/-1->4->5|5->4->6/-1/-1 [5] 5/-1/-1->4->6|6->4->5/-1/-1 [6] -1/-1/-1->4->7|7->4->-1/-1/-1 [7] -1/-1/-1->4->7|7->4->-1/-1/-1 [8] 7/-1/-1->4->0|0->4->7/-1/-1 [9] 7/-1/-1->4->0|0->4->7/-1/-1 [10] 6/-1/-1->4->5|5->4->6/-1/-1 [11] 5/-1/-1->4->6|6->4->5/-1/-1\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:53:53 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:941:941 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:54:54 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:53:53 [5] NCCL INFO Trees [0] 6/-1/-1->5->1|1->5->6/-1/-1 [1] 6/-1/-1->5->1|1->5->6/-1/-1 [2] 1/-1/-1->5->6|6->5->1/-1/-1 [3] 1/-1/-1->5->6|6->5->1/-1/-1 [4] 4/-1/-1->5->7|7->5->4/-1/-1 [5] 7/-1/-1->5->4|4->5->7/-1/-1 [6] 6/-1/-1->5->1|1->5->6/-1/-1 [7] 6/-1/-1->5->1|1->5->6/-1/-1 [8] 1/-1/-1->5->6|6->5->1/-1/-1 [9] 1/-1/-1->5->6|6->5->1/-1/-1 [10] 4/-1/-1->5->7|7->5->4/-1/-1 [11] 7/-1/-1->5->4|4->5->7/-1/-1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:941:941 [0] NCCL INFO Trees [0] 3/-1/-1->0->-1|-1->0->3/-1/-1 [1] 3/-1/-1->0->-1|-1->0->3/-1/-1 [2] 4/-1/-1->0->-1|-1->0->4/-1/-1 [3] 4/-1/-1->0->-1|-1->0->4/-1/-1 [4] 1/-1/-1->0->-1|-1->0->1/-1/-1 [5] 2/-1/-1->0->-1|-1->0->2/-1/-1 [6] 3/-1/-1->0->-1|-1->0->3/-1/-1 [7] 3/-1/-1->0->-1|-1->0->3/-1/-1 [8] 4/-1/-1->0->-1|-1->0->4/-1/-1 [9] 4/-1/-1->0->-1|-1->0->4/-1/-1 [10] 1/-1/-1->0->-1|-1->0->1/-1/-1 [11] 2/-1/-1->0->-1|-1->0->2/-1/-1\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:54:54 [6] NCCL INFO Trees [0] 7/-1/-1->6->5|5->6->7/-1/-1 [1] 7/-1/-1->6->5|5->6->7/-1/-1 [2] 5/-1/-1->6->7|7->6->5/-1/-1 [3] 5/-1/-1->6->7|7->6->5/-1/-1 [4] 2/-1/-1->6->4|4->6->2/-1/-1 [5] 4/-1/-1->6->2|2->6->4/-1/-1 [6] 7/-1/-1->6->5|5->6->7/-1/-1 [7] 7/-1/-1->6->5|5->6->7/-1/-1 [8] 5/-1/-1->6->7|7->6->5/-1/-1 [9] 5/-1/-1->6->7|7->6->5/-1/-1 [10] 2/-1/-1->6->4|4->6->2/-1/-1 [11] 4/-1/-1->6->2|2->6->4/-1/-1\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:55:55 [7] NCCL INFO Channel 00 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:46:46 [1] NCCL INFO Channel 00 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:47:47 [2] NCCL INFO Channel 00 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:49:49 [3] NCCL INFO Channel 00 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:51:51 [4] NCCL INFO Channel 00 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:53:53 [5] NCCL INFO Channel 00 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:941:941 [0] NCCL INFO Channel 00 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:54:54 [6] NCCL INFO Channel 00 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:51:51 [4] NCCL INFO Channel 00 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:55:55 [7] NCCL INFO Channel 00 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:47:47 [2] NCCL INFO Channel 00 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:46:46 [1] NCCL INFO Channel 00 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:49:49 [3] NCCL INFO Channel 00 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:53:53 [5] NCCL INFO Channel 00 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:54:54 [6] NCCL INFO Channel 00 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:51:51 [4] NCCL INFO Channel 01 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:941:941 [0] NCCL INFO Channel 01 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:55:55 [7] NCCL INFO Channel 01 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:47:47 [2] NCCL INFO Channel 01 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:46:46 [1] NCCL INFO Channel 01 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:49:49 [3] NCCL INFO Channel 01 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:53:53 [5] NCCL INFO Channel 01 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:54:54 [6] NCCL INFO Channel 01 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:51:51 [4] NCCL INFO Channel 01 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:46:46 [1] NCCL INFO Channel 01 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:47:47 [2] NCCL INFO Channel 01 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:55:55 [7] NCCL INFO Channel 01 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:49:49 [3] NCCL INFO Channel 01 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:53:53 [5] NCCL INFO Channel 01 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:54:54 [6] NCCL INFO Channel 01 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:51:51 [4] NCCL INFO Channel 02 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:941:941 [0] NCCL INFO Channel 02 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:55:55 [7] NCCL INFO Channel 02 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:46:46 [1] NCCL INFO Channel 02 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:47:47 [2] NCCL INFO Channel 02 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:49:49 [3] NCCL INFO Channel 02 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:53:53 [5] NCCL INFO Channel 02 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:54:54 [6] NCCL INFO Channel 02 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:51:51 [4] NCCL INFO Channel 02 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:49:49 [3] NCCL INFO Channel 02 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:55:55 [7] NCCL INFO Channel 02 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:47:47 [2] NCCL INFO Channel 02 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:46:46 [1] NCCL INFO Channel 02 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:53:53 [5] NCCL INFO Channel 02 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:54:54 [6] NCCL INFO Channel 02 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:941:941 [0] NCCL INFO Channel 03 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:49:49 [3] NCCL INFO Channel 03 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:51:51 [4] NCCL INFO Channel 03 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:46:46 [1] NCCL INFO Channel 03 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:55:55 [7] NCCL INFO Channel 03 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:47:47 [2] NCCL INFO Channel 03 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:53:53 [5] NCCL INFO Channel 03 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:54:54 [6] NCCL INFO Channel 03 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:49:49 [3] NCCL INFO Channel 03 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:51:51 [4] NCCL INFO Channel 03 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:47:47 [2] NCCL INFO Channel 03 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:55:55 [7] NCCL INFO Channel 03 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:46:46 [1] NCCL INFO Channel 03 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:53:53 [5] NCCL INFO Channel 03 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:54:54 [6] NCCL INFO Channel 03 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:49:49 [3] NCCL INFO Channel 04 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:941:941 [0] NCCL INFO Channel 04 : 0[170] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:51:51 [4] NCCL INFO Channel 04 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:47:47 [2] NCCL INFO Channel 04 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:55:55 [7] NCCL INFO Channel 04 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:46:46 [1] NCCL INFO Channel 04 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:53:53 [5] NCCL INFO Channel 04 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:54:54 [6] NCCL INFO Channel 04 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:47:47 [2] NCCL INFO Channel 04 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:49:49 [3] NCCL INFO Channel 04 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:51:51 [4] NCCL INFO Channel 04 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:55:55 [7] NCCL INFO Channel 04 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:46:46 [1] NCCL INFO Channel 04 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:53:53 [5] NCCL INFO Channel 04 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:54:54 [6] NCCL INFO Channel 04 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:47:47 [2] NCCL INFO Channel 05 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:941:941 [0] NCCL INFO Channel 05 : 0[170] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:49:49 [3] NCCL INFO Channel 05 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:51:51 [4] NCCL INFO Channel 05 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:46:46 [1] NCCL INFO Channel 05 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:55:55 [7] NCCL INFO Channel 05 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:53:53 [5] NCCL INFO Channel 05 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:54:54 [6] NCCL INFO Channel 05 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:46:46 [1] NCCL INFO Channel 05 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:49:49 [3] NCCL INFO Channel 05 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:47:47 [2] NCCL INFO Channel 05 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:51:51 [4] NCCL INFO Channel 05 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:55:55 [7] NCCL INFO Channel 05 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:53:53 [5] NCCL INFO Channel 05 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:54:54 [6] NCCL INFO Channel 05 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:46:46 [1] NCCL INFO Channel 06 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:941:941 [0] NCCL INFO Channel 06 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:49:49 [3] NCCL INFO Channel 06 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:51:51 [4] NCCL INFO Channel 06 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:47:47 [2] NCCL INFO Channel 06 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:55:55 [7] NCCL INFO Channel 06 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:53:53 [5] NCCL INFO Channel 06 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:54:54 [6] NCCL INFO Channel 06 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:51:51 [4] NCCL INFO Channel 06 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:46:46 [1] NCCL INFO Channel 06 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:49:49 [3] NCCL INFO Channel 06 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:47:47 [2] NCCL INFO Channel 06 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:55:55 [7] NCCL INFO Channel 06 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:53:53 [5] NCCL INFO Channel 06 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:54:54 [6] NCCL INFO Channel 06 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:51:51 [4] NCCL INFO Channel 07 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:941:941 [0] NCCL INFO Channel 07 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:49:49 [3] NCCL INFO Channel 07 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:46:46 [1] NCCL INFO Channel 07 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:47:47 [2] NCCL INFO Channel 07 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:55:55 [7] NCCL INFO Channel 07 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:53:53 [5] NCCL INFO Channel 07 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:54:54 [6] NCCL INFO Channel 07 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:51:51 [4] NCCL INFO Channel 07 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:49:49 [3] NCCL INFO Channel 07 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:47:47 [2] NCCL INFO Channel 07 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:46:46 [1] NCCL INFO Channel 07 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:55:55 [7] NCCL INFO Channel 07 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:53:53 [5] NCCL INFO Channel 07 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:54:54 [6] NCCL INFO Channel 07 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:51:51 [4] NCCL INFO Channel 08 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:941:941 [0] NCCL INFO Channel 08 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:49:49 [3] NCCL INFO Channel 08 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:46:46 [1] NCCL INFO Channel 08 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:47:47 [2] NCCL INFO Channel 08 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:55:55 [7] NCCL INFO Channel 08 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:53:53 [5] NCCL INFO Channel 08 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:54:54 [6] NCCL INFO Channel 08 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:49:49 [3] NCCL INFO Channel 08 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:51:51 [4] NCCL INFO Channel 08 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:46:46 [1] NCCL INFO Channel 08 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:47:47 [2] NCCL INFO Channel 08 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:55:55 [7] NCCL INFO Channel 08 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:53:53 [5] NCCL INFO Channel 08 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:54:54 [6] NCCL INFO Channel 08 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:49:49 [3] NCCL INFO Channel 09 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:941:941 [0] NCCL INFO Channel 09 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:51:51 [4] NCCL INFO Channel 09 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:46:46 [1] NCCL INFO Channel 09 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:47:47 [2] NCCL INFO Channel 09 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:55:55 [7] NCCL INFO Channel 09 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:53:53 [5] NCCL INFO Channel 09 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:54:54 [6] NCCL INFO Channel 09 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:49:49 [3] NCCL INFO Channel 09 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:51:51 [4] NCCL INFO Channel 09 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:47:47 [2] NCCL INFO Channel 09 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:46:46 [1] NCCL INFO Channel 09 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:55:55 [7] NCCL INFO Channel 09 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:53:53 [5] NCCL INFO Channel 09 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:54:54 [6] NCCL INFO Channel 09 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:49:49 [3] NCCL INFO Channel 10 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:941:941 [0] NCCL INFO Channel 10 : 0[170] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:51:51 [4] NCCL INFO Channel 10 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:46:46 [1] NCCL INFO Channel 10 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:47:47 [2] NCCL INFO Channel 10 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:55:55 [7] NCCL INFO Channel 10 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:53:53 [5] NCCL INFO Channel 10 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:54:54 [6] NCCL INFO Channel 10 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:47:47 [2] NCCL INFO Channel 10 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:49:49 [3] NCCL INFO Channel 10 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:51:51 [4] NCCL INFO Channel 10 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:46:46 [1] NCCL INFO Channel 10 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:55:55 [7] NCCL INFO Channel 10 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:53:53 [5] NCCL INFO Channel 10 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:54:54 [6] NCCL INFO Channel 10 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:47:47 [2] NCCL INFO Channel 11 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:941:941 [0] NCCL INFO Channel 11 : 0[170] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:49:49 [3] NCCL INFO Channel 11 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:46:46 [1] NCCL INFO Channel 11 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:51:51 [4] NCCL INFO Channel 11 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:55:55 [7] NCCL INFO Channel 11 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:53:53 [5] NCCL INFO Channel 11 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:54:54 [6] NCCL INFO Channel 11 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:46:46 [1] NCCL INFO Channel 11 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:49:49 [3] NCCL INFO Channel 11 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:47:47 [2] NCCL INFO Channel 11 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:51:51 [4] NCCL INFO Channel 11 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:55:55 [7] NCCL INFO Channel 11 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:53:53 [5] NCCL INFO Channel 11 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:46:46 [1] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:46:46 [1] NCCL INFO comm 0x56029977c510 rank 1 nranks 8 cudaDev 1 busId 180 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:54:54 [6] NCCL INFO Channel 11 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:941:941 [0] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:941:941 [0] NCCL INFO comm 0x557547f9d020 rank 0 nranks 8 cudaDev 0 busId 170 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:49:49 [3] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:49:49 [3] NCCL INFO comm 0x559dfb57a9b0 rank 3 nranks 8 cudaDev 3 busId 1a0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:51:51 [4] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:47:47 [2] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:51:51 [4] NCCL INFO comm 0x5561ce4d49f0 rank 4 nranks 8 cudaDev 4 busId 1b0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:55:55 [7] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:47:47 [2] NCCL INFO comm 0x5627c38b8260 rank 2 nranks 8 cudaDev 2 busId 190 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:55:55 [7] NCCL INFO comm 0x55a0887a5d10 rank 7 nranks 8 cudaDev 7 busId 1e0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:53:53 [5] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:53:53 [5] NCCL INFO comm 0x563f6f6f59f0 rank 5 nranks 8 cudaDev 5 busId 1c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:54:54 [6] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:54:54 [6] NCCL INFO comm 0x557e75316e10 rank 6 nranks 8 cudaDev 6 busId 1d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Running smdistributed.dataparallel v1.2.0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:01/17/2022 12:38:09 - WARNING - __main__ -   Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:01/17/2022 12:38:09 - INFO - __main__ -   Training/evaluation parameters Seq2SeqTrainingArguments(\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:_n_gpu=1,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:adafactor=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:adam_beta1=0.9,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:adam_beta2=0.999,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:adam_epsilon=1e-08,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:dataloader_drop_last=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:dataloader_num_workers=0,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:dataloader_pin_memory=True,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:ddp_find_unused_parameters=None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:debug=[],\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:deepspeed=None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:disable_tqdm=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:do_eval=True,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:do_predict=True,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:do_train=True,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:eval_accumulation_steps=None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:eval_steps=None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:evaluation_strategy=IntervalStrategy.NO,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:fp16=True,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:fp16_backend=auto,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:fp16_full_eval=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:fp16_opt_level=O1,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:generation_max_length=None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:generation_num_beams=None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:gradient_accumulation_steps=1,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:gradient_checkpointing=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:greater_is_better=None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:group_by_length=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:hub_model_id=None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:hub_strategy=HubStrategy.EVERY_SAVE,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:hub_token=<HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:ignore_data_skip=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:label_names=None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:label_smoothing_factor=0.0,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:learning_rate=5e-05,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:length_column_name=length,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:load_best_model_at_end=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:local_rank=0,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:log_level=-1,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:log_level_replica=-1,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:log_on_each_node=True,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:logging_dir=/opt/ml/model/runs/Jan17_12-38-01_algo-1,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:logging_first_step=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:logging_nan_inf_filter=True,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:logging_steps=500,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:logging_strategy=IntervalStrategy.STEPS,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:lr_scheduler_type=SchedulerType.LINEAR,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:max_grad_norm=1.0,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:max_steps=-1,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:metric_for_best_model=None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:mp_parameters=,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:no_cuda=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:num_train_epochs=3.0,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:output_dir=/opt/ml/model,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:overwrite_output_dir=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:past_index=-1,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:per_device_eval_batch_size=1,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:per_device_train_batch_size=1,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:predict_with_generate=True,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:prediction_loss_only=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:push_to_hub=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:push_to_hub_model_id=None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:push_to_hub_organization=None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:remove_unused_columns=True,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:report_to=[],\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:resume_from_checkpoint=None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:run_name=/opt/ml/model,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:save_on_each_node=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:save_steps=500,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:save_strategy=IntervalStrategy.STEPS,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:save_total_limit=None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:seed=7,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:sharded_ddp=[],\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:skip_memory_metrics=True,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:sortish_sampler=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:tpu_metrics_debug=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:tpu_num_cores=None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:use_legacy_prediction_loop=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:warmup_ratio=0.0,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:warmup_steps=0,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:weight_decay=0.0,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:xpu_backend=None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:01/17/2022 12:38:09 - WARNING - __main__ -   Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:01/17/2022 12:38:09 - WARNING - __main__ -   Process rank: 4, device: cuda:4, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:01/17/2022 12:38:09 - WARNING - __main__ -   Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:01/17/2022 12:38:09 - WARNING - __main__ -   Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:01/17/2022 12:38:09 - WARNING - __main__ -   Process rank: 5, device: cuda:5, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:01/17/2022 12:38:09 - WARNING - __main__ -   Process rank: 7, device: cuda:7, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:01/17/2022 12:38:09 - WARNING - __main__ -   Process rank: 6, device: cuda:6, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:   0%|          | 0.00/2.18k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading: 5.36kB [00:00, 4.70MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading:   0%|          | 0.00/2.18k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading: 5.36kB [00:00, 4.59MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading:   0%|          | 0.00/2.18k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading: 5.36kB [00:00, 4.74MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:   0%|          | 0.00/1.86k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading: 20.9kB [00:00, 19.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Downloading:   0%|          | 0.00/2.18k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Downloading: 5.36kB [00:00, 5.00MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:    return _run_code(code, main_globals, None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/__main__.py\", line 7, in <module>\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:    main()\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/run.py\", line 196, in main\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:    run_command_line(args)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/run.py\", line 47, in run_command_line\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:    run_path(sys.argv[0], run_name='__main__')\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 265, in run_path\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:    return _run_module_code(code, init_globals, run_name,\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 97, in _run_module_code\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:    _run_code(code, mod_globals, init_globals,\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  File \"run_summarization.py\", line 606, in <module>\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:    main()\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  File \"run_summarization.py\", line 313, in main\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:    datasets = load_dataset(data_args.dataset_name, data_args.dataset_config_name, cache_dir=model_args.cache_dir)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/site-packages/datasets/load.py\", line 1632, in load_dataset\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:    builder_instance.download_and_prepare(\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/site-packages/datasets/builder.py\", line 548, in download_and_prepare\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:    raise IOError(\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:OSError: Not enough disk space. Needed: 11.77 GiB (download: 6.01 GiB, generated: 5.77 GiB, post-processed: Unknown size)\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34mMPI_ABORT was invoked on rank 6 in communicator MPI COMMUNICATOR 5 DUP FROM 0\u001b[0m\n",
      "\u001b[34mwith errorcode 1.\u001b[0m\n",
      "\u001b[34mNOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.\u001b[0m\n",
      "\u001b[34mYou may or may not see output from other processes, depending on\u001b[0m\n",
      "\u001b[34mexactly when Open MPI kills them.\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    return _run_code(code, main_globals, None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/__main__.py\", line 7, in <module>\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    main()\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/run.py\", line 196, in main\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    run_command_line(args)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/run.py\", line 47, in run_command_line\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    run_path(sys.argv[0], run_name='__main__')\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 265, in run_path\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    return _run_module_code(code, init_globals, run_name,\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 97, in _run_module_code\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    _run_code(code, mod_globals, init_globals,\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"run_summarization.py\", line 606, in <module>\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    main()\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"run_summarization.py\", line 313, in main\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    datasets = load_dataset(data_args.dataset_name, data_args.dataset_config_name, cache_dir=model_args.cache_dir)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/site-packages/datasets/load.py\", line 1632, in load_dataset\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    builder_instance.download_and_prepare(\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/site-packages/datasets/builder.py\", line 548, in download_and_prepare\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:    raise IOError(\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:OSError: Not enough disk space. Needed: 11.77 GiB (download: 6.01 GiB, generated: 5.77 GiB, post-processed: Unknown size)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:    return _run_code(code, main_globals, None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/__main__.py\", line 7, in <module>\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:    main()\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/run.py\", line 196, in main\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:    run_command_line(args)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/run.py\", line 47, in run_command_line\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:    run_path(sys.argv[0], run_name='__main__')\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 265, in run_path\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:    return _run_module_code(code, init_globals, run_name,\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 97, in _run_module_code\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:    _run_code(code, mod_globals, init_globals,\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  File \"run_summarization.py\", line 606, in <module>\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:    main()\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  File \"run_summarization.py\", line 313, in main\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:    datasets = load_dataset(data_args.dataset_name, data_args.dataset_config_name, cache_dir=model_args.cache_dir)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/site-packages/datasets/load.py\", line 1632, in load_dataset\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:    builder_instance.download_and_prepare(\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/site-packages/datasets/builder.py\", line 548, in download_and_prepare\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:    raise IOError(\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:OSError: Not enough disk space. Needed: 11.77 GiB (download: 6.01 GiB, generated: 5.77 GiB, post-processed: Unknown size)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    return _run_code(code, main_globals, None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/__main__.py\", line 7, in <module>\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    main()\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/run.py\", line 196, in main\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    run_command_line(args)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/run.py\", line 47, in run_command_line\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    run_path(sys.argv[0], run_name='__main__')\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 265, in run_path\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    return _run_module_code(code, init_globals, run_name,\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 97, in _run_module_code\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    _run_code(code, mod_globals, init_globals,\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"run_summarization.py\", line 606, in <module>\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    main()\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"run_summarization.py\", line 313, in main\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    datasets = load_dataset(data_args.dataset_name, data_args.dataset_config_name, cache_dir=model_args.cache_dir)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/site-packages/datasets/load.py\", line 1632, in load_dataset\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    builder_instance.download_and_prepare(\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/site-packages/datasets/builder.py\", line 548, in download_and_prepare\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:    raise IOError(\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:OSError: Not enough disk space. Needed: 11.77 GiB (download: 6.01 GiB, generated: 5.77 GiB, post-processed: Unknown size)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:    return _run_code(code, main_globals, None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/__main__.py\", line 7, in <module>\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:    main()\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/run.py\", line 196, in main\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:    run_command_line(args)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/run.py\", line 47, in run_command_line\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:    run_path(sys.argv[0], run_name='__main__')\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 265, in run_path\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:    return _run_module_code(code, init_globals, run_name,\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 97, in _run_module_code\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:    _run_code(code, mod_globals, init_globals,\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  File \"run_summarization.py\", line 606, in <module>\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:    main()\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  File \"run_summarization.py\", line 313, in main\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:    datasets = load_dataset(data_args.dataset_name, data_args.dataset_config_name, cache_dir=model_args.cache_dir)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/site-packages/datasets/load.py\", line 1632, in load_dataset\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:    builder_instance.download_and_prepare(\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/site-packages/datasets/builder.py\", line 548, in download_and_prepare\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:    raise IOError(\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:OSError: Not enough disk space. Needed: 11.77 GiB (download: 6.01 GiB, generated: 5.77 GiB, post-processed: Unknown size)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    return _run_code(code, main_globals, None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/__main__.py\", line 7, in <module>\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    main()\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/run.py\", line 196, in main\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    run_command_line(args)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/run.py\", line 47, in run_command_line\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    run_path(sys.argv[0], run_name='__main__')\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 265, in run_path\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    return _run_module_code(code, init_globals, run_name,\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 97, in _run_module_code\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    _run_code(code, mod_globals, init_globals,\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"run_summarization.py\", line 606, in <module>\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    main()\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"run_summarization.py\", line 313, in main\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    datasets = load_dataset(data_args.dataset_name, data_args.dataset_config_name, cache_dir=model_args.cache_dir)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/site-packages/datasets/load.py\", line 1632, in load_dataset\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    builder_instance.download_and_prepare(\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/site-packages/datasets/builder.py\", line 548, in download_and_prepare\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:    raise IOError(\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:OSError: Not enough disk space. Needed: 11.77 GiB (download: 6.01 GiB, generated: 5.77 GiB, post-processed: Unknown size)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    return _run_code(code, main_globals, None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/__main__.py\", line 7, in <module>\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    main()\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/run.py\", line 196, in main\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    run_command_line(args)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/run.py\", line 47, in run_command_line\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    run_path(sys.argv[0], run_name='__main__')\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 265, in run_path\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    return _run_module_code(code, init_globals, run_name,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 97, in _run_module_code\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    _run_code(code, mod_globals, init_globals,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"run_summarization.py\", line 606, in <module>\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    main()\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"run_summarization.py\", line 313, in main\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    datasets = load_dataset(data_args.dataset_name, data_args.dataset_config_name, cache_dir=model_args.cache_dir)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/site-packages/datasets/load.py\", line 1632, in load_dataset\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    builder_instance.download_and_prepare(\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  File \"/opt/conda/lib/python3.8/site-packages/datasets/builder.py\", line 548, in download_and_prepare\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    raise IOError(\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:OSError: Not enough disk space. Needed: 11.77 GiB (download: 6.01 GiB, generated: 5.77 GiB, post-processed: Unknown size)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   0%|          | 0.00/2.18k [00:00<?, ?B/s]#015Downloading: 5.36kB [00:00, 4.59MB/s]                   \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading:   0%|          | 0.00/2.18k [00:00<?, ?B/s]#015Downloading: 5.36kB [00:00, 5.04MB/s]                   \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading:   0%|          | 0.00/2.18k [00:00<?, ?B/s]#015Downloading: 5.36kB [00:00, 4.50MB/s]                   \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading:   0%|          | 0.00/1.86k [00:00<?, ?B/s]#015Downloading: 20.9kB [00:00, 12.7MB/s]                   \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading:   0%|          | 0.00/1.86k [00:00<?, ?B/s]#015Downloading: 20.9kB [00:00, 18.5MB/s]                   \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading:   0%|          | 0.00/2.18k [00:00<?, ?B/s]#015Downloading: 5.36kB [00:00, 5.11MB/s]\u001b[0m\n",
      "\u001b[34m[algo-1:00034] 6 more processes have sent help message help-mpi-api.txt / mpi-abort\u001b[0m\n",
      "\u001b[34m[algo-1:00034] Set MCA parameter \"orte_base_help_aggregate\" to 0 to see all help / error messages\u001b[0m\n",
      "\u001b[34m2022-01-17 12:38:10,298 sagemaker-training-toolkit ERROR    Reporting training FAILURE\u001b[0m\n",
      "\u001b[34m2022-01-17 12:38:10,298 sagemaker-training-toolkit ERROR    ExecuteUserScriptError:\u001b[0m\n",
      "\u001b[34mExitCode 1\u001b[0m\n",
      "\u001b[34mErrorMessage \"\"\u001b[0m\n",
      "\u001b[34mCommand \"mpirun --host algo-1 -np 8 --allow-run-as-root --tag-output --oversubscribe -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -mca plm_rsh_num_concurrent 1 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x SMDATAPARALLEL_USE_SINGLENODE=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1 -x LD_PRELOAD=/opt/conda/lib/python3.8/site-packages/gethostname.cpython-38-x86_64-linux-gnu.so smddprun /opt/conda/bin/python3.8 -m mpi4py run_summarization.py --cache_dir opt/ml/input --dataset_config_name g --dataset_name big_patent --do_eval True --do_predict True --do_train True --fp16 True --learning_rate 5e-05 --model_name_or_path facebook/bart-large-cnn --num_train_epochs 3 --output_dir /opt/ml/model --per_device_eval_batch_size 1 --per_device_train_batch_size 1 --predict_with_generate True --seed 7 --summary_column abstract --text_column description\"\u001b[0m\n",
      "\u001b[34m2022-01-17 12:38:10,298 sagemaker-training-toolkit ERROR    Encountered exit_code 1\u001b[0m\n",
      "\n",
      "2022-01-17 12:38:18 Uploading - Uploading generated training model\n",
      "2022-01-17 12:39:01 Failed - Training job failed\n",
      "ProfilerReport-1642422525: Stopping\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Training job huggingface-pytorch-training-2022-01-17-12-28-45-434: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nExitCode 1\nErrorMessage \"\"\nCommand \"mpirun --host algo-1 -np 8 --allow-run-as-root --tag-output --oversubscribe -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -mca plm_rsh_num_concurrent 1 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x SMDATAPARALLEL_USE_SINGLENODE=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1 -x LD_PRELOAD=/opt/conda/lib/python3.8/site-packages/gethostname.cpython-38-x86_64-linux-gnu.so smddprun /opt/conda/bin/python3.8 -m mpi4py run_summarization.py --cache_dir opt/ml/input --dataset_config_name g --dataset_name big_patent --do_eval True --do_predict True --do_train True --fp16 True --learning_rate 5e-05 --model_name_or_path facebook/bart-large-cnn --num_train_epochs 3 --output_dir /opt/ml/model --per_device_eval_batch_size 1 --per_device_train_batch_size 1 --predict_with_generate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-7899b08002c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# starting the train job\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhuggingface_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#huggingface_estimator.fit({'train': 's3://robcost-bigpatent/train.tar.gz','test': 's3://robcost-bigpatent/test.tar.gz'})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_compilation_job_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   1650\u001b[0m         \u001b[0;31m# If logs are requested, call logs_for_jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1651\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"None\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1652\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1653\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1654\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mlogs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type)\u001b[0m\n\u001b[1;32m   3776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3777\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3778\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_job_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TrainingJobStatus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3779\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3780\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_check_job_status\u001b[0;34m(self, job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   3333\u001b[0m                 ),\n\u001b[1;32m   3334\u001b[0m                 \u001b[0mallowed_statuses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Completed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Stopped\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3335\u001b[0;31m                 \u001b[0mactual_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3336\u001b[0m             )\n\u001b[1;32m   3337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Training job huggingface-pytorch-training-2022-01-17-12-28-45-434: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nExitCode 1\nErrorMessage \"\"\nCommand \"mpirun --host algo-1 -np 8 --allow-run-as-root --tag-output --oversubscribe -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -mca plm_rsh_num_concurrent 1 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x SMDATAPARALLEL_USE_SINGLENODE=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1 -x LD_PRELOAD=/opt/conda/lib/python3.8/site-packages/gethostname.cpython-38-x86_64-linux-gnu.so smddprun /opt/conda/bin/python3.8 -m mpi4py run_summarization.py --cache_dir opt/ml/input --dataset_config_name g --dataset_name big_patent --do_eval True --do_predict True --do_train True --fp16 True --learning_rate 5e-05 --model_name_or_path facebook/bart-large-cnn --num_train_epochs 3 --output_dir /opt/ml/model --per_device_eval_batch_size 1 --per_device_train_batch_size 1 --predict_with_generate"
     ]
    }
   ],
   "source": [
    "# starting the train job\n",
    "huggingface_estimator.fit()\n",
    "#huggingface_estimator.fit({'train': 's3://robcost-bigpatent/train.tar.gz','test': 's3://robcost-bigpatent/test.tar.gz'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying the endpoint\n",
    "\n",
    "To deploy our endpoint, we call `deploy()` on our HuggingFace estimator object, passing in our desired number of instances and instance type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = huggingface_estimator.deploy(1,\"ml.g4dn.xlarge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we use the returned predictor object to call the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patent = '''BACKGROUND OF THE DISCLOSURE\n",
    "\n",
    "      1. Technical Field of the Disclosure\n",
    "      This embodiment relates in general to electric cars. More specifically, the preferred embodiment relates to an electric car having a vertical row of seats.\n",
    "      2. Description of the Related Art\n",
    "      An electric car is powered by an electric motor using electrical energy stored in batteries or other charged devices. Electric cars are environment friendly as it do not produce any harmful gases such as carbon monoxide, organic compounds, hydro carbons etc. Electric cars are economical because of very low maintenance and operating costs.\n",
    "      Conventional electric cars have considerable drawbacks. For example, an existing electric car includes an electric drive and at least one connected electrical energy storage module. A guide extends longitudinally along the motor vehicle, and supports the storage module in a manner relative to the motor vehicle. Conventional electric cars employ horizontal rows of seats consuming excess space.\n",
    "      Another existing electric vehicle is capable of carrying at least two passengers and has at least three wheels. Passengers sit in tandem and most of the batteries or fuel cell systems are located to the sides of the passengers. This vehicle has an aerodynamically shaped body with substantially reduced frontal area and drag. The body is lightweight, made from shock absorbing materials and structures, and has pressure-airless tires, which enhances the safety of the passengers. The vehicle also includes an advanced hydrogen-electric hybrid propulsion system with quick refueling from existing infrastructure and various additional optional features and systems. However the batteries or fuel cell systems are placed to the sides of the passengers which increases the overall the dimension and weight of the vehicle.\n",
    "      Yet another existing electrical car embodiment is comprised of bodywork with ground-engaging wheels for vehicle motion over the ground, the bodywork contains an electric motor to drive the vehicle via the wheels and batteries to power the electric motor.\n",
    "      This embodiment provides an additional energy generation means comprised of a tunnel extending through the bodywork. It includes a turbine fan/alternator set located in the tunnel at the rear of the vehicle where electrical energy is generated during vehicle motion to charge the batteries. This results in improved performance of the vehicle, especially with regard to its range. The inlet to the tunnel at the vehicle front constitutes the major portion of the vehicles frontal area. A special alternator is provided, and the vehicle can also include a solar cell means for battery charging. However, the seats are placed far apart which consumes a lot of space.\n",
    "      Hence, it can be seen, that there is a need for an electric car that contains a vertical rows of seats. This needed electric car would save more space than existing models to form a vehicle of smaller size with more passengers. Moreover, this needed electric car would consume less power and use a less bulky charging means during transportation. The present embodiment accomplishes these objectives.\n",
    "SUMMARY OF THE DISCLOSURE\n",
    "\n",
    "      To minimize the limitations found in the prior art, and to minimize other limitations that will be apparent upon the reading of the specifications, the present invention provides an electric car having a vertical row of seat for accommodating at least one passenger. The electric car comprises a center console placed at an interior forepart of the electric car. A steering is attached to the center console. A rectangular seat is mounted on a rectangular box, the rectangular box being longitudinally placed at a center part of the electric car. A storage area is enclosed within the rectangular box, the storage area includes a personal storage and a battery storage. The battery storage includes a battery pack having a set of rechargeable batteries. The battery pack is used to power the electric car. A plug point is located at a rear end of the electric car for charging the battery pack. A pair of rotatable front wheels and a pair of rotatable back wheels are provided for ensuring smooth movement of the electric car. Both the center console and the steering of the electric car are aligned with the rectangular seat. The rectangular seat is mounted on the rectangular box having the storage area in order to save space. The electric car is configured to have a compact seating arrangement. A back support may be provided for a driver's rectangular seat. The electric car is specially designed to achieve better performance by increasing the energy efficiency and reducing the power consumption. The at least one passenger can be seated facing front direction with one leg on each side of the rectangular seat. The electric car is capable of accommodating more passengers with relatively small size and with minimum power consumption during transportation.\n",
    "      In alternate embodiment of the present invention at least one step and ladder seat is positioned at the center part of the electric car. The electric car comprises a car body having a front end, a rear end, a top portion and a bottom portion. A center console placed at an interior forepart of the electric car. A steering is attached to the center console. At least one step and ladder seat is positioned at the center part of the electric car for accommodating at least one passenger. In addition to the at least one step and ladder seat, a horizontal row of seats for accommodating a pair of passengers is located at the rear end of the vehicle. A battery storage having a battery pack is positioned at the bottom portion of step and ladder seat for powering the electric car. A plug point is located at a rear end of the electric car for charging the battery pack. A personal storage is placed at the rear end of horizontal row of seat.\n",
    "      One objective of the invention is to provide an electric car capable of accommodating four or more passengers with relatively smaller vehicle size.\n",
    "      Another objective of the invention is to provide an electric car with compact seating arrangement.\n",
    "      A third objective of the invention is to provide an electric car with better performance by increasing the energy efficiency and reducing the power consumption.\n",
    "      These and other advantages and features of the present invention are described with specificity so as to make the present invention understandable to one of ordinary skill in the art.'''\n",
    "\n",
    "data= {\"inputs\":patent}\n",
    "\n",
    "predictor.predict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we delete the endpoint again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the fine-tuned model to [huggingface.co](http://huggingface.co)\n",
    "\n",
    "We can download our model from Amazon S3 and unzip it using the following snippet.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "from sagemaker.s3 import S3Downloader\n",
    "\n",
    "local_path = 'my_bart_model'\n",
    "\n",
    "os.makedirs(local_path, exist_ok = True)\n",
    "\n",
    "# download model from S3\n",
    "S3Downloader.download(\n",
    "    s3_uri=huggingface_estimator.model_data, # s3 uri where the trained model is located\n",
    "    local_path=local_path, # local path where *.targ.gz is saved\n",
    "    sagemaker_session=sess # sagemaker session used for training the model\n",
    ")\n",
    "\n",
    "# unzip model\n",
    "tar = tarfile.open(f\"{local_path}/model.tar.gz\", \"r:gz\")\n",
    "tar.extractall(path=local_path)\n",
    "tar.close()\n",
    "os.remove(f\"{local_path}/model.tar.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we are going to upload our model to [huggingface.co](http://huggingface.co) we need to create a `model_card`. The `model_card` describes the model includes hyperparameters, results and which dataset was used for training. To create a `model_card` we create a `README.md` in our `local_path`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read eval and test results \n",
    "with open(f\"{local_path}/eval_results.json\") as f:\n",
    "    eval_results_raw = json.load(f)\n",
    "    eval_results={}\n",
    "    eval_results[\"eval_rouge1\"] = eval_results_raw[\"eval_rouge1\"]\n",
    "    eval_results[\"eval_rouge2\"] = eval_results_raw[\"eval_rouge2\"]\n",
    "    eval_results[\"eval_rougeL\"] = eval_results_raw[\"eval_rougeL\"]\n",
    "    eval_results[\"eval_rougeLsum\"] = eval_results_raw[\"eval_rougeLsum\"]\n",
    "\n",
    "with open(f\"{local_path}/test_results.json\") as f:\n",
    "    test_results_raw = json.load(f)\n",
    "    test_results={}\n",
    "    test_results[\"test_rouge1\"] = test_results_raw[\"test_rouge1\"]\n",
    "    test_results[\"test_rouge2\"] = test_results_raw[\"test_rouge2\"]\n",
    "    test_results[\"test_rougeL\"] = test_results_raw[\"test_rougeL\"]\n",
    "    test_results[\"test_rougeLsum\"] = test_results_raw[\"test_rougeLsum\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "After we extract all the metrics we want to include we are going to create our `README.md`. Additionally to the automated generation of the results table we add the metrics manually to the `metadata` of our model card under `model-index`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eval_results)\n",
    "print(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "MODEL_CARD_TEMPLATE = \"\"\"\n",
    "---\n",
    "language: en\n",
    "tags:\n",
    "- sagemaker\n",
    "- bart\n",
    "- summarization\n",
    "license: apache-2.0\n",
    "datasets:\n",
    "- samsum\n",
    "model-index:\n",
    "- name: {model_name}\n",
    "  results:\n",
    "  - task: \n",
    "      name: Abstractive Text Summarization\n",
    "      type: abstractive-text-summarization\n",
    "    dataset:\n",
    "      name: \"SAMSum Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization\" \n",
    "      type: samsum\n",
    "    metrics:\n",
    "       - name: Validation ROGUE-1\n",
    "         type: rogue-1\n",
    "         value: 42.621\n",
    "       - name: Validation ROGUE-2\n",
    "         type: rogue-2\n",
    "         value: 21.9825\n",
    "       - name: Validation ROGUE-L\n",
    "         type: rogue-l\n",
    "         value: 33.034\n",
    "       - name: Test ROGUE-1\n",
    "         type: rogue-1\n",
    "         value: 41.3174\n",
    "       - name: Test ROGUE-2\n",
    "         type: rogue-2\n",
    "         value: 20.8716\n",
    "       - name: Test ROGUE-L\n",
    "         type: rogue-l\n",
    "         value: 32.1337\n",
    "widget:\n",
    "- text: | \n",
    "    Jeff: Can I train a ðŸ¤— Transformers model on Amazon SageMaker? \n",
    "    Philipp: Sure you can use the new Hugging Face Deep Learning Container. \n",
    "    Jeff: ok.\n",
    "    Jeff: and how can I get started? \n",
    "    Jeff: where can I find documentation? \n",
    "    Philipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face \n",
    "---\n",
    "## `{model_name}`\n",
    "This model was trained using Amazon SageMaker and the new Hugging Face Deep Learning container.\n",
    "For more information look at:\n",
    "- [ðŸ¤— Transformers Documentation: Amazon SageMaker](https://huggingface.co/transformers/sagemaker.html)\n",
    "- [Example Notebooks](https://github.com/huggingface/notebooks/tree/master/sagemaker)\n",
    "- [Amazon SageMaker documentation for Hugging Face](https://docs.aws.amazon.com/sagemaker/latest/dg/hugging-face.html)\n",
    "- [Python SDK SageMaker documentation for Hugging Face](https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/index.html)\n",
    "- [Deep Learning Container](https://github.com/aws/deep-learning-containers/blob/master/available_images.md#huggingface-training-containers)\n",
    "## Hyperparameters\n",
    "    {hyperparameters}\n",
    "## Usage\n",
    "    from transformers import pipeline\n",
    "    summarizer = pipeline(\"summarization\", model=\"philschmid/{model_name}\")\n",
    "    conversation = '''Jeff: Can I train a ðŸ¤— Transformers model on Amazon SageMaker? \n",
    "    Philipp: Sure you can use the new Hugging Face Deep Learning Container. \n",
    "    Jeff: ok.\n",
    "    Jeff: and how can I get started? \n",
    "    Jeff: where can I find documentation? \n",
    "    Philipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face                                           \n",
    "    '''\n",
    "    nlp(conversation)\n",
    "## Results\n",
    "| key | value |\n",
    "| --- | ----- |\n",
    "{eval_table}\n",
    "{test_table}\n",
    "\"\"\"\n",
    "\n",
    "# Generate model card (todo: add more data from Trainer)\n",
    "model_card = MODEL_CARD_TEMPLATE.format(\n",
    "    model_name=f\"{hyperparameters['model_name_or_path'].split('/')[1]}-{hyperparameters['dataset_name']}\",\n",
    "    hyperparameters=json.dumps(hyperparameters, indent=4, sort_keys=True),\n",
    "    eval_table=\"\\n\".join(f\"| {k} | {v} |\" for k, v in eval_results.items()),\n",
    "    test_table=\"\\n\".join(f\"| {k} | {v} |\" for k, v in test_results.items()),\n",
    ")\n",
    "with open(f\"{local_path}/README.md\", \"w\") as f:\n",
    "    f.write(model_card)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have our unzipped model and model card located in `my_bart_model` we can use the either `huggingface_hub` SDK to create a repository and upload it to [huggingface.co](http://huggingface.co) or go to https://huggingface.co/new an create a new repository and upload it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "from huggingface_hub import HfApi, Repository\n",
    "\n",
    "hf_username = \"philschmid\" # your username on huggingface.co\n",
    "hf_email = \"philipp@huggingface.co\" # email used for commit\n",
    "repository_name = f\"{hyperparameters['model_name_or_path'].split('/')[1]}-{hyperparameters['dataset_name']}\" # repository name on huggingface.co\n",
    "password = getpass(\"Enter your password:\") # creates a prompt for entering password\n",
    "\n",
    "# get hf token\n",
    "token = HfApi().login(username=hf_username, password=password)\n",
    "\n",
    "# create repository\n",
    "repo_url = HfApi().create_repo(token=token, name=repository_name, exist_ok=True)\n",
    "\n",
    "# create a Repository instance\n",
    "model_repo = Repository(use_auth_token=token,\n",
    "                        clone_from=repo_url,\n",
    "                        local_dir=local_path,\n",
    "                        git_user=hf_username,\n",
    "                        git_email=hf_email)\n",
    "\n",
    "# push model to the hub\n",
    "model_repo.push_to_hub()\n",
    "\n",
    "print(f\"https://huggingface.co/{hf_username}/{repository_name}\")"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "interpreter": {
   "hash": "c281c456f1b8161c8906f4af2c08ed2c40c50136979eaae69688b01f70e9f4a9"
  },
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
