# hf-summ-sagemaker


run summ args

[1,2]<stderr>:usage: run_summarization.py [-h] --model_name_or_path MODEL_NAME_OR_PATH
[1,2]<stderr>:                            [--config_name CONFIG_NAME]
[1,2]<stderr>:                            [--tokenizer_name TOKENIZER_NAME]
[1,2]<stderr>:                            [--cache_dir CACHE_DIR] [--no_use_fast_tokenizer]
[1,2]<stderr>:                            [--use_fast_tokenizer [USE_FAST_TOKENIZER]]
[1,2]<stderr>:                            [--model_revision MODEL_REVISION]
[1,2]<stderr>:                            [--use_auth_token [USE_AUTH_TOKEN]]
[1,2]<stderr>:                            [--dataset_name DATASET_NAME]
[1,2]<stderr>:                            [--dataset_config_name DATASET_CONFIG_NAME]
[1,2]<stderr>:                            [--text_column TEXT_COLUMN]
[1,2]<stderr>:                            [--summary_column SUMMARY_COLUMN]
[1,2]<stderr>:                            [--train_file TRAIN_FILE]
[1,2]<stderr>:                            [--validation_file VALIDATION_FILE]
[1,2]<stderr>:                            [--test_file TEST_FILE]
[1,2]<stderr>:                            [--overwrite_cache [OVERWRITE_CACHE]]
[1,2]<stderr>:                            [--preprocessing_num_workers PREPROCESSING_NUM_WORKERS]
[1,2]<stderr>:                            [--max_source_length MAX_SOURCE_LENGTH]
[1,2]<stderr>:                            [--max_target_length MAX_TARGET_LENGTH]
[1,2]<stderr>:                            [--val_max_target_length VAL_MAX_TARGET_LENGTH]
[1,2]<stderr>:                            [--pad_to_max_length [PAD_TO_MAX_LENGTH]]
[1,2]<stderr>:                            [--max_train_samples MAX_TRAIN_SAMPLES]
[1,2]<stderr>:                            [--max_eval_samples MAX_EVAL_SAMPLES]
[1,2]<stderr>:                            [--max_predict_samples MAX_PREDICT_SAMPLES]
[1,2]<stderr>:                            [--num_beams NUM_BEAMS]
[1,2]<stderr>:                            [--no_ignore_pad_token_for_loss]
[1,2]<stderr>:                            [--ignore_pad_token_for_loss [IGNORE_PAD_TOKEN_FOR_LOSS]]
[1,2]<stderr>:                            [--source_prefix SOURCE_PREFIX] --output_dir
[1,2]<stderr>:                            OUTPUT_DIR
[1,2]<stderr>:                            [--overwrite_output_dir [OVERWRITE_OUTPUT_DIR]]
[1,2]<stderr>:                            [--do_train [DO_TRAIN]] [--do_eval [DO_EVAL]]
[1,2]<stderr>:                            [--do_predict [DO_PREDICT]]
[1,2]<stderr>:                            [--evaluation_strategy {no,steps,epoch}]
[1,2]<stderr>:                            [--prediction_loss_only [PREDICTION_LOSS_ONLY]]
[1,2]<stderr>:                            [--per_device_train_batch_size PER_DEVICE_TRAIN_BATCH_SIZE]
[1,2]<stderr>:                            [--per_device_eval_batch_size PER_DEVICE_EVAL_BATCH_SIZE]
[1,2]<stderr>:                            [--per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE]
[1,2]<stderr>:                            [--per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE]
[1,2]<stderr>:                            [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]
[1,2]<stderr>:                            [--eval_accumulation_steps EVAL_ACCUMULATION_STEPS]
[1,2]<stderr>:                            [--learning_rate LEARNING_RATE]
[1,2]<stderr>:                            [--weight_decay WEIGHT_DECAY]
[1,2]<stderr>:                            [--adam_beta1 ADAM_BETA1]
[1,2]<stderr>:                            [--adam_beta2 ADAM_BETA2]
[1,2]<stderr>:                            [--adam_epsilon ADAM_EPSILON]
[1,2]<stderr>:                            [--max_grad_norm MAX_GRAD_NORM]
[1,2]<stderr>:                            [--num_train_epochs NUM_TRAIN_EPOCHS]
[1,2]<stderr>:                            [--max_steps MAX_STEPS]
[1,2]<stderr>:                            [--lr_scheduler_type {linear,cosine,cosine_with_restarts,polynomial,constant,constant_with_warmup}]
[1,2]<stderr>:                            [--warmup_ratio WARMUP_RATIO]
[1,2]<stderr>:                            [--warmup_steps WARMUP_STEPS]
[1,2]<stderr>:                            [--logging_dir LOGGING_DIR]
[1,2]<stderr>:                            [--logging_strategy {no,steps,epoch}]
[1,2]<stderr>:                            [--logging_first_step [LOGGING_FIRST_STEP]]